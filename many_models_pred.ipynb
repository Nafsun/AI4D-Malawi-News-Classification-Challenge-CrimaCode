{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_AASHwXxg</td>\n",
       "      <td>Mwangonde: Khansala wachinyamata Akamati achi...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_AGoFySzn</td>\n",
       "      <td>MCP siidakhutire ndi kalembera Chipani cha Ma...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_AGrrkBGP</td>\n",
       "      <td>Bungwe la MANEPO Lapempha Boma Liganizire Anth...</td>\n",
       "      <td>HEALTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_AIJeigeG</td>\n",
       "      <td>Ndale zogawanitsa miyambo zanyanya Si zachile...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_APMprMbV</td>\n",
       "      <td>Nanga wapolisi ataphofomoka? Masiku ano sichi...</td>\n",
       "      <td>LAW/ORDER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                               Text      Label\n",
       "0  ID_AASHwXxg   Mwangonde: Khansala wachinyamata Akamati achi...   POLITICS\n",
       "1  ID_AGoFySzn   MCP siidakhutire ndi kalembera Chipani cha Ma...   POLITICS\n",
       "2  ID_AGrrkBGP  Bungwe la MANEPO Lapempha Boma Liganizire Anth...     HEALTH\n",
       "3  ID_AIJeigeG   Ndale zogawanitsa miyambo zanyanya Si zachile...   POLITICS\n",
       "4  ID_APMprMbV   Nanga wapolisi ataphofomoka? Masiku ano sichi...  LAW/ORDER"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(df):\n",
    "    import re\n",
    "    counter = 0\n",
    "    df = df.copy()\n",
    "    #open the stop word file\n",
    "    stop_word = \"\"\n",
    "    with open(\"stop_nyanja_words.txt\", \"r\") as f:\n",
    "        stop_word = f.read()\n",
    "    #tokenize the stop words\n",
    "    from nltk import word_tokenize\n",
    "    stop_word = list(set(word_tokenize(stop_word)))\n",
    "    for i in df:\n",
    "        #tokenize words of the article\n",
    "        word_tokens = word_tokenize(i)\n",
    "        #remove the stopwords\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_word] \n",
    "        #join the sentence\n",
    "        filtered_sentence = \" \".join([x for x in filtered_sentence])\n",
    "        #remove all numbers\n",
    "        filtered_sentence = re.sub(\"\\d+\", \"\", filtered_sentence)\n",
    "        #replace the current version with the filtered one\n",
    "        df[counter] = filtered_sentence\n",
    "        counter += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_shuffled_data(train_data, trunc):\n",
    "    train_data_label = train_data[\"Label\"].copy()\n",
    "    train_data_text = train_data[\"Text\"].copy()\n",
    "    from nltk import word_tokenize\n",
    "    for i in range(len(train_data_text)):\n",
    "        #tokenize the sentence into words\n",
    "        shuf = word_tokenize(train_data_text[i])\n",
    "        #truncate words\n",
    "        if(trunc < len(shuf)):\n",
    "            shuf = shuf[:trunc]\n",
    "            #replace the current text with the truncated text\n",
    "            train_data_text[i] = \" \".join(z for z in shuf)\n",
    "        else:\n",
    "            del train_data_text[i]\n",
    "            del train_data_label[i]\n",
    "    train_data_text = pd.DataFrame({\"Text\": train_data_text, \"Label\": train_data_label}, columns=[\"Text\", \"Label\"])\n",
    "    return train_data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def articles_preprocessing(df):\n",
    "    #convert to lowercase\n",
    "    df['Text'] = df['Text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    #remove punctuations\n",
    "    df['Text'] = df['Text'].str.replace('[^\\w\\s]', '')\n",
    "    #remove stopwords\n",
    "    df['Text'] = removeStopWords(df['Text'])        \n",
    "    \n",
    "    #convert labels into numbers\n",
    "    df[\"Label\"] = le.transform(df[\"Label\"])\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    \n",
    "    #add new shuffled data\n",
    "    df_1 = new_shuffled_data(df, 100)\n",
    "    df_2 = new_shuffled_data(df, 200)\n",
    "    df_3 = new_shuffled_data(df, 300)\n",
    "    df_4 = new_shuffled_data(df, 400)\n",
    "    df_5 = new_shuffled_data(df, 500)\n",
    "    df_6 = new_shuffled_data(df, 600)\n",
    "    df_7 = new_shuffled_data(df, 700)\n",
    "    df_8 = new_shuffled_data(df, 800)\n",
    "    df_9 = new_shuffled_data(df, 900)\n",
    "    df_10 = new_shuffled_data(df, 1000)\n",
    "    df_11 = new_shuffled_data(df, 1100)\n",
    "    df_12 = new_shuffled_data(df, 1200)\n",
    "    df_13 = new_shuffled_data(df, 1300)\n",
    "    df_14 = new_shuffled_data(df, 1400)\n",
    "    df_15 = new_shuffled_data(df, 1500)\n",
    "    df_16 = new_shuffled_data(df, 1600)\n",
    "    df_17 = new_shuffled_data(df, 1700)\n",
    "    df_18 = new_shuffled_data(df, 1800)\n",
    "    df_19 = new_shuffled_data(df, 1900)\n",
    "    df_20 = new_shuffled_data(df, 2000)\n",
    "    df_21 = new_shuffled_data(df, 2100)\n",
    "    df_22 = new_shuffled_data(df, 2200)\n",
    "    df_23 = new_shuffled_data(df, 2300)\n",
    "    df_24 = new_shuffled_data(df, 2400)\n",
    "    df_25 = new_shuffled_data(df, 2500)\n",
    "    df_26 = new_shuffled_data(df, 2600)\n",
    "    df_27 = new_shuffled_data(df, 2700)\n",
    "    df_28 = new_shuffled_data(df, 2800)\n",
    "    df_29 = new_shuffled_data(df, 2900)\n",
    "    df_30 = new_shuffled_data(df, 3000)\n",
    "    df_31 = new_shuffled_data(df, 3100)\n",
    "    df_32 = new_shuffled_data(df, 3200)\n",
    "    df_33 = new_shuffled_data(df, 3300)\n",
    "    df_34 = new_shuffled_data(df, 3400)\n",
    "    df_35 = new_shuffled_data(df, 3500)\n",
    "    df_36 = new_shuffled_data(df, 3600)\n",
    "    df_37 = new_shuffled_data(df, 3700)\n",
    "    df_38 = new_shuffled_data(df, 3800)\n",
    "    df_39 = new_shuffled_data(df, 3900)\n",
    "    df_40 = new_shuffled_data(df, 4000)\n",
    "    \n",
    "    df = pd.concat([df, df_1, df_2, df_3, df_4, df_5, df_6, df_7, \n",
    "                          df_8, df_9, df_10, df_11, df_12, df_13, df_14, df_15, \n",
    "                          df_16, df_17, df_18, df_19, df_20, df_21, df_22, df_23, \n",
    "                          df_24, df_25, df_26, df_27, df_28, df_29, df_30, df_31,\n",
    "                          df_32, df_33, df_34, df_35, df_36, df_37, df_38, df_39, \n",
    "                          df_40], ignore_index=True)\n",
    "\n",
    "    from sklearn import model_selection\n",
    "    df[\"kfold\"] = -1\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    y = df[\"Label\"]\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "    \n",
    "    all_data = df[\"Text\"]\n",
    "\n",
    "    df_train = df[df.kfold != rand].reset_index(drop=True)\n",
    "\n",
    "    df_valid = df[df.kfold == rand].reset_index(drop=True)\n",
    "\n",
    "    x_train = df_train.drop([\"ID\", \"Label\", \"kfold\"], axis=1)\n",
    "    y_train = df_train.Label.values\n",
    "\n",
    "    x_valid = df_valid.drop([\"ID\", \"Label\", \"kfold\"], axis=1)\n",
    "    y_valid = df_valid.Label.values\n",
    "    \n",
    "    x_train = x_train[\"Text\"]\n",
    "    x_valid = x_valid[\"Text\"]\n",
    "\n",
    "    return x_train, y_train, x_valid, y_valid, all_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid, all_data = articles_preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer()"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "#vectorizer = TfidfVectorizer()\n",
    "#vectorizer.fit(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import joblib\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf.bin']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#joblib.dump(vectorizer, \"tfidf.bin\")\n",
    "vectorizer = joblib.load(\"tfidf.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tfidf = vectorizer.transform(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.array([])\n",
    "counter = 0\n",
    "\n",
    "for i in os.listdir():\n",
    "    if(i.endswith(\"bin\") == True):\n",
    "        if(\"tfidf.bin\" != i):\n",
    "            model = joblib.load(i)\n",
    "            if(counter == 0):\n",
    "                y_preds = model.predict_proba(new_tfidf)\n",
    "            else:\n",
    "                y_preds = y_preds + model.predict_proba(new_tfidf)\n",
    "            print(i)\n",
    "            counter += 1\n",
    "\n",
    "overall = y_preds / counter\n",
    "\n",
    "save_pos = []\n",
    "counter = 0\n",
    "\n",
    "for i in range(len(overall[:, 1])):\n",
    "    l = list(overall[counter])\n",
    "    m = max(l)\n",
    "    save_pos.append(l.index(m))\n",
    "    counter += 1\n",
    "counter = 0\n",
    "\n",
    "y_preds = save_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_valid, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def articles_preprocessing(df, label=None, stratify=None):\n",
    "    #convert to lowercase\n",
    "    df['Text'] = df['Text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    #remove punctuations\n",
    "    df['Text'] = df['Text'].str.replace('[^\\w\\s]', '')\n",
    "    #remove stopwords\n",
    "    df['Text'] = removeStopWords(df['Text'])        \n",
    "    return df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = articles_preprocessing(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tfidf = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = np.array([])\n",
    "counter = 0\n",
    "\n",
    "for i in os.listdir():\n",
    "    if(i.endswith(\"bin\") == True):\n",
    "        if(\"tfidf.bin\" != i):\n",
    "            model = joblib.load(i)\n",
    "            if(counter == 0):\n",
    "                y_preds = model.predict_proba(new_tfidf)\n",
    "            else:\n",
    "                y_preds = y_preds + model.predict_proba(new_tfidf)\n",
    "            print(i)\n",
    "            counter += 1\n",
    "\n",
    "overall = y_preds / counter\n",
    "\n",
    "save_pos = []\n",
    "counter = 0\n",
    "\n",
    "for i in range(len(overall[:, 1])):\n",
    "    l = list(overall[counter])\n",
    "    m = max(l)\n",
    "    save_pos.append(l.index(m))\n",
    "    counter += 1\n",
    "counter = 0\n",
    "\n",
    "prediction = save_pos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = pd.DataFrame(np.c_[df_test.ID, prediction], columns=[\"ID\", \"Label\"]).set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {0: 'ARTS AND CRAFTS', 1: 'CULTURE', 2: 'ECONOMY', 3: 'EDUCATION', 4: 'FARMING', 5: 'FLOODING',\n",
    " 6: 'HEALTH', 7: 'LAW/ORDER', 8: 'LOCALCHIEFS', 9: 'MUSIC', 10: 'OPINION/ESSAY', 11: 'POLITICS',\n",
    " 12: 'RELATIONSHIPS', 13: 'RELIGION', 14: 'SOCIAL', 15: 'SOCIAL ISSUES', 16: 'SPORTS', 17: 'TRANSPORT',\n",
    " 18: 'WILDLIFE/ENVIRONMENT', 19: 'WITCHCRAFT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df.Label = pd_df.Label.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df.to_csv(\"new_pred_4.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
