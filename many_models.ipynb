{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Text</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ID_AASHwXxg</td>\n",
       "      <td>Mwangonde: Khansala wachinyamata Akamati achi...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ID_AGoFySzn</td>\n",
       "      <td>MCP siidakhutire ndi kalembera Chipani cha Ma...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ID_AGrrkBGP</td>\n",
       "      <td>Bungwe la MANEPO Lapempha Boma Liganizire Anth...</td>\n",
       "      <td>HEALTH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ID_AIJeigeG</td>\n",
       "      <td>Ndale zogawanitsa miyambo zanyanya Si zachile...</td>\n",
       "      <td>POLITICS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ID_APMprMbV</td>\n",
       "      <td>Nanga wapolisi ataphofomoka? Masiku ano sichi...</td>\n",
       "      <td>LAW/ORDER</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            ID                                               Text      Label\n",
       "0  ID_AASHwXxg   Mwangonde: Khansala wachinyamata Akamati achi...   POLITICS\n",
       "1  ID_AGoFySzn   MCP siidakhutire ndi kalembera Chipani cha Ma...   POLITICS\n",
       "2  ID_AGrrkBGP  Bungwe la MANEPO Lapempha Boma Liganizire Anth...     HEALTH\n",
       "3  ID_AIJeigeG   Ndale zogawanitsa miyambo zanyanya Si zachile...   POLITICS\n",
       "4  ID_APMprMbV   Nanga wapolisi ataphofomoka? Masiku ano sichi...  LAW/ORDER"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LabelEncoder()"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.fit(df[\"Label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removeStopWords(df):\n",
    "    import re\n",
    "    counter = 0\n",
    "    df = df.copy()\n",
    "    #open the stop word file\n",
    "    stop_word = \"\"\n",
    "    with open(\"stop_nyanja_words.txt\", \"r\") as f:\n",
    "        stop_word = f.read()\n",
    "    #tokenize the stop words\n",
    "    from nltk import word_tokenize\n",
    "    stop_word = list(set(word_tokenize(stop_word)))\n",
    "    for i in df:\n",
    "        #tokenize words of the article\n",
    "        word_tokens = word_tokenize(i)\n",
    "        #remove the stopwords\n",
    "        filtered_sentence = [w for w in word_tokens if not w in stop_word] \n",
    "        #join the sentence\n",
    "        filtered_sentence = \" \".join([x for x in filtered_sentence])\n",
    "        #remove all numbers\n",
    "        filtered_sentence = re.sub(\"\\d+\", \"\", filtered_sentence)\n",
    "        #replace the current version with the filtered one\n",
    "        df[counter] = filtered_sentence\n",
    "        counter += 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand = 0\n",
    "randomstate = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_shuffled_data(train_data, trunc):\n",
    "    train_data_label = train_data[\"Label\"].copy()\n",
    "    train_data_text = train_data[\"Text\"].copy()\n",
    "    from nltk import word_tokenize\n",
    "    for i in range(len(train_data_text)):\n",
    "        #tokenize the sentence into words\n",
    "        shuf = word_tokenize(train_data_text[i])\n",
    "        #truncate words\n",
    "        if(trunc < len(shuf)):\n",
    "            shuf = shuf[:trunc]\n",
    "            #replace the current text with the truncated text\n",
    "            train_data_text[i] = \" \".join(z for z in shuf)\n",
    "        else:\n",
    "            del train_data_text[i]\n",
    "            del train_data_label[i]\n",
    "    train_data_text = pd.DataFrame({\"Text\": train_data_text, \"Label\": train_data_label}, columns=[\"Text\", \"Label\"])\n",
    "    return train_data_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def articles_preprocessing(df):\n",
    "    #convert to lowercase\n",
    "    df['Text'] = df['Text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    #remove punctuations\n",
    "    df['Text'] = df['Text'].str.replace('[^\\w\\s]', '')\n",
    "    #remove stopwords\n",
    "    df['Text'] = removeStopWords(df['Text'])   \n",
    "    \n",
    "    #convert labels into numbers\n",
    "    df[\"Label\"] = le.transform(df[\"Label\"])\n",
    "    df = df.sample(frac=1, random_state=randomstate).reset_index(drop=True)\n",
    "    \n",
    "    from sklearn import model_selection\n",
    "    df[\"kfold\"] = -1\n",
    "    df = df.sample(frac=1, random_state=randomstate).reset_index(drop=True)\n",
    "    y = df[\"Label\"]\n",
    "    kf = model_selection.StratifiedKFold(n_splits=5, random_state=randomstate, shuffle=True)\n",
    "    for f, (t_, v_) in enumerate(kf.split(X=df, y=y)):\n",
    "        df.loc[v_, 'kfold'] = f\n",
    "\n",
    "    df_train = df[df.kfold != rand].reset_index(drop=True)\n",
    "    \n",
    "    df_train = df_train.drop([\"ID\", \"kfold\"], axis=1)\n",
    "\n",
    "    #add new shuffled data\n",
    "    df_train_1 = new_shuffled_data(df_train, 100)\n",
    "    df_train_2 = new_shuffled_data(df_train, 200)\n",
    "    df_train_3 = new_shuffled_data(df_train, 300)\n",
    "    df_train_4 = new_shuffled_data(df_train, 400)\n",
    "    df_train_5 = new_shuffled_data(df_train, 500)\n",
    "    df_train_6 = new_shuffled_data(df_train, 600)\n",
    "    df_train_7 = new_shuffled_data(df_train, 700)\n",
    "    df_train_8 = new_shuffled_data(df_train, 800)\n",
    "    df_train_9 = new_shuffled_data(df_train, 900)\n",
    "    df_train_10 = new_shuffled_data(df_train, 1000)\n",
    "    df_train_11 = new_shuffled_data(df_train, 1100)\n",
    "    df_train_12 = new_shuffled_data(df_train, 1200)\n",
    "    df_train_13 = new_shuffled_data(df_train, 1300)\n",
    "    df_train_14 = new_shuffled_data(df_train, 1400)\n",
    "    df_train_15 = new_shuffled_data(df_train, 1500)\n",
    "    df_train_16 = new_shuffled_data(df_train, 1600)\n",
    "    df_train_17 = new_shuffled_data(df_train, 1700)\n",
    "    df_train_18 = new_shuffled_data(df_train, 1800)\n",
    "    df_train_19 = new_shuffled_data(df_train, 1900)\n",
    "    df_train_20 = new_shuffled_data(df_train, 2000)\n",
    "    df_train_21 = new_shuffled_data(df_train, 2100)\n",
    "    df_train_22 = new_shuffled_data(df_train, 2200)\n",
    "    df_train_23 = new_shuffled_data(df_train, 2300)\n",
    "    df_train_24 = new_shuffled_data(df_train, 2400)\n",
    "    df_train_25 = new_shuffled_data(df_train, 2500)\n",
    "    df_train_26 = new_shuffled_data(df_train, 2600)\n",
    "    df_train_27 = new_shuffled_data(df_train, 2700)\n",
    "    df_train_28 = new_shuffled_data(df_train, 2800)\n",
    "    df_train_29 = new_shuffled_data(df_train, 2900)\n",
    "    df_train_30 = new_shuffled_data(df_train, 3000)\n",
    "    df_train_31 = new_shuffled_data(df_train, 3100)\n",
    "    df_train_32 = new_shuffled_data(df_train, 3200)\n",
    "    df_train_33 = new_shuffled_data(df_train, 3300)\n",
    "    df_train_34 = new_shuffled_data(df_train, 3400)\n",
    "    df_train_35 = new_shuffled_data(df_train, 3500)\n",
    "    df_train_36 = new_shuffled_data(df_train, 3600)\n",
    "    df_train_37 = new_shuffled_data(df_train, 3700)\n",
    "    df_train_38 = new_shuffled_data(df_train, 3800)\n",
    "    df_train_39 = new_shuffled_data(df_train, 3900)\n",
    "    df_train_40 = new_shuffled_data(df_train, 4000)\n",
    "    \n",
    "    df_train = pd.concat([df_train, df_train_1, df_train_2, df_train_3, df_train_4, df_train_5, df_train_6, df_train_7, \n",
    "                          df_train_8, df_train_9, df_train_10, df_train_11, df_train_12, df_train_13, df_train_14, df_train_15, \n",
    "                          df_train_16, df_train_17, df_train_18, df_train_19, df_train_20, df_train_21, df_train_22, df_train_23, \n",
    "                          df_train_24, df_train_25, df_train_26, df_train_27, df_train_28, df_train_29, df_train_30, df_train_31,\n",
    "                          df_train_32, df_train_33, df_train_34, df_train_35, df_train_36, df_train_37, df_train_38, df_train_39, \n",
    "                          df_train_40], ignore_index=True)\n",
    "    \n",
    "    df_train = df_train.sample(frac=1, random_state=randomstate).reset_index(drop=True)\n",
    "    \n",
    "    df_valid = df[df.kfold == rand].reset_index(drop=True)\n",
    "\n",
    "    x_train = df_train.drop([\"Label\"], axis=1)\n",
    "    y_train = df_train.Label.values\n",
    "\n",
    "    x_valid = df_valid.drop([\"ID\", \"Label\", \"kfold\"], axis=1)\n",
    "    y_valid = df_valid.Label.values\n",
    "    \n",
    "    x_train = x_train[\"Text\"]\n",
    "    x_valid = x_valid[\"Text\"]\n",
    "\n",
    "    return x_train, y_train, x_valid, y_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_valid, y_valid = articles_preprocessing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "913"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(x_train[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = joblib.load(\"tfidf.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_data = vectorizer.transform(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2541x49245 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 320416 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model = MultinomialNB(alpha=0.001).fit(tfidf_data, y_train)\n",
    "#model = XGBClassifier(random_state=randomstate, max_depth=20, n_estimators=40, use_label_encoder=False, objective=\"multi:softmax\", gamma=0.1, learning_rate=0.3).fit(tfidf_data, y_train, eval_metric=\"merror\")\n",
    "model = LogisticRegression(random_state=randomstate, tol=5, penalty=\"l2\", C=10, solver=\"lbfgs\", multi_class=\"multinomial\", max_iter=100, class_weight=\"balanced\").fit(tfidf_data, y_train)\n",
    "#model = RandomForestClassifier(random_state=randomstate, n_estimators=1000).fit(tfidf_data, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tfidf = vectorizer.transform(x_valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<288x49245 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 39401 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = model.predict(new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_valid, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6354166666666666"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy # 0.6354166666666666"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv(\"Test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def articles_preprocessing(df):\n",
    "    #convert to lowercase\n",
    "    df['Text'] = df['Text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))\n",
    "    #remove punctuations\n",
    "    df['Text'] = df['Text'].str.replace('[^\\w\\s]', '')\n",
    "    #remove stopwords\n",
    "    df['Text'] = removeStopWords(df['Text'])        \n",
    "    return df['Text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test = articles_preprocessing(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_tfidf = vectorizer.transform(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.predict(new_tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df = pd.DataFrame(np.c_[df_test.ID, prediction], columns=[\"ID\", \"Label\"]).set_index(\"ID\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapper = {0: 'ARTS AND CRAFTS', 1: 'CULTURE', 2: 'ECONOMY', 3: 'EDUCATION', 4: 'FARMING', 5: 'FLOODING',\n",
    " 6: 'HEALTH', 7: 'LAW/ORDER', 8: 'LOCALCHIEFS', 9: 'MUSIC', 10: 'OPINION/ESSAY', 11: 'POLITICS',\n",
    " 12: 'RELATIONSHIPS', 13: 'RELIGION', 14: 'SOCIAL', 15: 'SOCIAL ISSUES', 16: 'SPORTS', 17: 'TRANSPORT',\n",
    " 18: 'WILDLIFE/ENVIRONMENT', 19: 'WITCHCRAFT'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd_df.Label = pd_df.Label.map(mapper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ID\n",
       "ID_ADHEtjTi    SOCIAL ISSUES\n",
       "ID_AHfJktdQ         RELIGION\n",
       "ID_AUJIHpZr    RELATIONSHIPS\n",
       "ID_AUKYBbIM    SOCIAL ISSUES\n",
       "ID_AZnsVPEi           HEALTH\n",
       "                   ...      \n",
       "ID_zdpOUWyJ        LAW/ORDER\n",
       "ID_zhnOomuu    RELATIONSHIPS\n",
       "ID_zmWHvBJb        LAW/ORDER\n",
       "ID_zphjdFIb    SOCIAL ISSUES\n",
       "ID_ztdtrNxt         POLITICS\n",
       "Name: Label, Length: 620, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd_df.Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#joblib.dump(model, f\"LogisticRegression-{rand}-{accuracy.round(3)}-validate-{randomstate}.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
